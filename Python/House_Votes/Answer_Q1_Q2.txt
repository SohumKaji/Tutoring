1. For each method of evaluating missing data, for both Decision Trees and Naive Bayes, I've listed each score from the 5-fold cross validation 
for precision, f1, and recall. The average of those 5 scores is listed below to give further perspective. The decision tree used a random state of 5 so that these
results could be replicated.


i) Discard Missing
Decision_Tree: 
test_precision [ 0.91666667  0.91666667  1.          0.95        0.86363636]
Average: 0.929393939394
test_f1 [ 0.95652174  0.95652174  0.97674419  0.92682927  0.88372093]
Average: 0.940067572567
test_recall [ 1.          1.          0.95454545  0.9047619   0.9047619 ]
Average: 0.952813852814

Naive Bayes:
score_time [ 0.00401998  0.00414515  0.00476098  0.003613    0.00355697]
Average: 0.00401921272278
fit_time [ 0.00199699  0.00234389  0.00195408  0.00170302  0.00160289]
Average: 0.00192017555237
test_precision [ 0.91304348  1.          1.          1.          0.86956522]
Average: 0.95652173913
test_f1 [ 0.93333333  0.92682927  0.97674419  1.          0.90909091]
Average: 0.949199539353
test_recall [ 0.95454545  0.86363636  0.95454545  1.          0.95238095]
Average: 0.945021645022

ii) Treat Missing as Value
Decision_Tree: 
test_precision [ 0.97058824  0.97058824  0.96969697  0.93548387  0.9375    ]
Average: 0.956771462251
test_f1 [ 0.97058824  0.97058824  0.95522388  0.90625     0.92307692]
Average: 0.945145454852
test_recall [ 0.97058824  0.97058824  0.94117647  0.87878788  0.90909091]
Average: 0.934046345811

Naive Bayes:
test_precision [ 0.88888889  0.94117647  0.96666667  1.          0.83333333]
Average: 0.926013071895
test_f1 [ 0.91428571  0.94117647  0.90625     0.96875     0.86956522]
Average: 0.920005480453
test_recall [ 0.94117647  0.94117647  0.85294118  0.93939394  0.90909091]
Average: 0.916755793226

iii) Impute Missing
Decision_Tree: 
test_precision [ 0.96969697  0.87179487  0.96969697  0.93548387  0.81578947]
Average: 0.912492431168
test_f1 [ 0.95522388  0.93150685  0.95522388  0.90625     0.87323944]
Average: 0.924288809426
test_recall [ 0.94117647  1.          0.94117647  0.87878788  0.93939394]
Average: 0.940106951872

Naive Bayes:
test_precision [ 0.89189189  0.81578947  0.96774194  0.93939394  0.78378378]
Average: 0.879720204848
test_f1 [ 0.92957746  0.86111111  0.92307692  0.93939394  0.82857143]
Average: 0.896346173388
test_recall [ 0.97058824  0.91176471  0.88235294  0.93939394  0.87878788]
Average: 0.916577540107

2. What type of dataset would you choose decision trees as a classifier over Naive Bayes. Vice versa?

I would choose decision trees over naive bayes as a classifier if the data may have outliars or may not be linearly separable. The reasoning for this is that, Decision trees are non-parametric so they do not make assumptions about the defining factors of the data. A weakensses of decision trees is that they tend to overfit the data. This becomes more of an issue if our training data is not representative of the real world. I would stick to using decision trees on data that does a good job representing the real world.

I would choose naive bayes over decision trees as a classifier if the dataset is small. Naive bayes works best if the data is conditionally
independent. Relatedly, Naive bayes is poor at determining interactions between features so I would avoid using Naive Bayes in scenarios like that. So in summary: smaller datasets where the features are conditionally independent from each other.